{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfef9b4-b2c1-4ec9-ab4d-5f7486a74ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall cmake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53129bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.list_builders()\n",
    "print(\"Available Datasets:\", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for datasets containing the keyword \"imdb_reviews\"\n",
    "keyword = \"imdb_reviews\"\n",
    "filtered_datasets = [ds for ds in datasets if keyword in ds]\n",
    "print(\"Filtered Datasets:\", filtered_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "data, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Check dataset metadata\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", info.splits['train'].num_examples)\n",
    "print(\"Number of testing examples:\", info.splits['test'].num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Check the available splits (train, test, etc.)\n",
    "train_data = data['train']  # This gives you the training data\n",
    "test_data = data['test']    # This gives you the test data\n",
    "\n",
    "# Optionally print the dataset info\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract examples from the train dataset\n",
    "train_examples = [{'Text': text.numpy().decode('utf-8'), 'Sentiment': int(label.numpy())} for text, label in train_data]\n",
    "df = pd.DataFrame(train_examples)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(train_examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1d2d1",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the DataFrame:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in the DataFrame:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated entries:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the duplicated rows\n",
    "print(\"Duplicated rows:\\n\", df[df.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Check the shape after removing duplicates\n",
    "print(\"Shape after removing duplicates:\", df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that there are no duplicates left\n",
    "print(\"Duplicated rows after removing:\", df_cleaned.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed information about the DataFrame (data types, non-null counts)\n",
    "print(\"Dataset info:\")\n",
    "\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c3331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cd97f6",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf780ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text (remove URLs, mentions, special characters, etc.)\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove Twitter mentions (@username)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remove non-alphabetical characters and keep spaces (e.g., punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Optionally, convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df_cleaned['Text'] = df_cleaned['Text'].apply(clean_text)\n",
    "\n",
    "# Display the first few cleaned rows\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt  # Import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "# Download the NLTK punkt tokenizer if not already done\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the words\n",
    "all_words = ' '.join(df_cleaned['Text'])\n",
    "words = word_tokenize(all_words)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "common_words = word_counts.most_common(10)\n",
    "\n",
    "# Prepare data for plotting\n",
    "words, counts = zip(*common_words)\n",
    "\n",
    "# Plot the top 10 words\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
    "plt.title(\"Top 10 Most Frequent Words in Reviews\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to store the length of each review\n",
    "df_cleaned['Review_Length'] = df_cleaned['Text'].apply(len)\n",
    "\n",
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['Review_Length'], kde=True, color='blue', bins=30)\n",
    "plt.title(\"Distribution of Review Lengths\")\n",
    "plt.xlabel(\"Review Length (Number of Characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830d12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Review Length vs Sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Review_Length', y='Sentiment', data=df_cleaned, hue='Sentiment', palette='coolwarm', alpha=0.6)\n",
    "plt.title(\"Sentiment vs Review Length\")\n",
    "plt.xlabel(\"Review Length (Number of Characters)\")\n",
    "plt.ylabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot: Review Length vs Sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Sentiment', y='Review_Length', data=df_cleaned, palette='coolwarm')\n",
    "plt.title(\"Review Length Distribution by Sentiment\")\n",
    "plt.xlabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.ylabel(\"Review Length (Number of Characters)\")\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c820456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pairplot to visualize relationships between columns in the dataset\n",
    "sns.pairplot(df_cleaned[['Review_Length', 'Sentiment']])\n",
    "plt.title(\"Pairplot of Review Length vs Sentiment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a193f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a maximum threshold for review length (e.g., 2000 characters)\n",
    "max_review_length = 2000\n",
    "\n",
    "# Remove reviews with length exceeding the threshold\n",
    "df_cleaned = df_cleaned[df_cleaned['Review_Length'] <= max_review_length]\n",
    "\n",
    "# Check the new shape of the dataset\n",
    "print(\"Shape after removing outliers:\", df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Review Length after cleaning:\", df_cleaned['Review_Length'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd2f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "df_cleaned['Text'] = df_cleaned['Text'].apply(remove_stopwords)\n",
    "print(\"Sample text without stopwords:\", df_cleaned['Text'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Process only if it's a string\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    return text  # Return the input unchanged if not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentiment labels (assuming the sentiment column is named 'Sentiment')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_cleaned['Sentiment'] = encoder.fit_transform(df_cleaned['Sentiment'])\n",
    "\n",
    "# Verify encoding\n",
    "print(df_cleaned['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature (Text) and Target (Sentiment)\n",
    "X = df_cleaned['Text']\n",
    "y = df_cleaned['Sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data size: {len(X_train)}\")\n",
    "print(f\"Testing data size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abab7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape (training): {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF matrix shape (testing): {X_test_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a846b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Check training accuracy\n",
    "train_accuracy = model.score(X_train_tfidf, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b971c6",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_accuracy = model.score(X_test_tfidf, y_test)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test labels\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6786b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41bf5c6f",
   "metadata": {},
   "source": [
    "# ML Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7825ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming your data preprocessing and train-test split code are already done:\n",
    "# X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "# Start MLFlow run for logging\n",
    "with mlflow.start_run():\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    mlflow.log_param(\"max_iter\", 100)  # Example: max_iter for Logistic Regression\n",
    "    \n",
    "    # Train the Logistic Regression model (this can be the model you're already training)\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions and calculate accuracy (you can keep this as your evaluation step)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Log the accuracy metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log the trained model (you can also log the model as you're doing here)\n",
    "    mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "    \n",
    "    # Optionally: Log confusion matrix as an artifact (use your existing confusion matrix code)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    \n",
    "    # Save confusion matrix as an image and log it as an artifact\n",
    "    cm_path = \"confusion_matrix.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    \n",
    "    print(f\"Model trained and logged with accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
